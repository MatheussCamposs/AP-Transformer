SKETCH - Decoder-only transformer (Arithmetic Progression)

LEGEND

  B = batch size
  L = sequence length in tokens (after token convertion) used by the transformer
  N = number of arithmetic progression terms in the context (before becoming tokens)
  D = fixed number of digits per term (left zero-padding)
  d_model = embedding dimension
  V = vocabulary size

VOCABULARY

  Tokens = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', ' '}
  |Tokens| = 11

  Note: the numbers will be represented using zero padding over a fixed size D. So a number like 7 for D = 3 will be interpreted as 007. The main reason is to keep the vocabulary size small while still flexible for big numbers and sequences.

TRANSFORMER FLOW

0. Dataset generation

  The dataset will be generated randomly, the sequences will have different sizes aswell as different common differences. After generation the sequence will be converted to a string applying the zero left padding and appended to the dataset.txt file. Estimated size of the dataset will be of 10.000 sequences of sizes ranging from 2-100. And common difference ranging from 1-500.

  files: generate.py, conversor.py, dataset.txt

1. Tokenizer
  
  Input: string s representing the sequence.
  Output: integer ids (0-10)

  Here, the tokenizer its used only for mapping the tokens with their respective ids (or the reverse).

  files: tokenizer.py, vocab.txt

2. Embedding
  
  Input: ids(B, L)
  Output: X (B, L, d_model)

  - Trainable table E (v, d_model)
  - X = E[ids]
  
  file: layers/embedding.py

3. Positional encoding (sin/cos)

  Input: X (B, L, d_model)
  Output: X_pos (B, L, d_model)

  - X_pos = X + PE(L, d_model)
  - PE is deterministic, no backprop needed.

  file: layers/positional_encoding.py

4. TransformerBlock(Single Head, Layer Norm, MLP)

  Input: X(B, L, d_model)
  Output: enriched X(B, L, d_model)

  4.1 Masked single-head self-attention
    Q = Z @ Wq
    K = Z @ Wk
    V = Z @ Wv

    Where Wq, Wk and Wv are trainable.

    Shapes:
      d_k = d_model
      Q, K, V -> (B, L, d_model)

    Scores:
      S = (Q @ K ^ T) / sqrt(d_k) -> (B, L, L)

    Masking:
      block S[:, i, j] when j > i, so that in training the tranformer doesnt 'cheat'.

  4.2 LayerNorm #1

    Z = LN(X)

  4.3 FFN (Feed Forward Network)
    
    M = Linear(d_model -> d_ff)
    M = ReLU(M)
    M = Linear(d_ff -> d_model)

    Result: M (B, L, d_model)
  
  files: model/attention_single_head.py
         model/ffn.py
         model/transformer_block.py
         layers/linear.py
         layers/layernorm.py
         layers/activations.py
         model/mask.py

5. Output projection (linear -> vocabulary)
  Input: X (B, L, d_model)
  Output: logits (B, L, V)

    logits = X @ W_out + b
    W_out (d_model, V), b(V)

    file: output_head.py

6.Traning loss: Softmax + Cross-Entropy

  Input: 
    logits (B, L, V)
    targets: y_ids (B, L)
  Output: scalar loss 

  file: layers/softmax_ce.py
